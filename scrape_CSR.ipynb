{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: scrape NIH CSR site for study section members\n",
    "\n",
    "data source: https://public.csr.nih.gov/StudySections/StandingStudySections\n",
    "\n",
    "example site: https://public.era.nih.gov/pubroster/preRosIndex.era?AGENDA=438116&CID=102353\n",
    "\n",
    "https://beautiful-soup-4.readthedocs.io/en/latest/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD:\n",
    "# # Nested for loop version, keeping original chunks separate below because I still need to figure out how to extract the section ID and date info\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "\n",
    "# driver = webdriver.Firefox()\n",
    "\n",
    "# target_url = 'https://public.csr.nih.gov/StudySections/StandingStudySections'\n",
    "\n",
    "# driver.get(target_url)\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# urls = [item.get(\"href\") for item in soup.find_all(\"a\")]\n",
    "# urls_final = [x for x in urls if x.startswith('/StudySections')]\n",
    "\n",
    "# full_urls = []\n",
    "# for ending in urls_final[10:-8]:\n",
    "#     full_urls.append('https://public.csr.nih.gov'+ending)\n",
    "\n",
    "# member_list = list()\n",
    "# for url in full_urls:\n",
    "#     driver.get(url)\n",
    "#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#     roster_urls = [item.get(\"href\") for item in soup.find_all(\"a\")]\n",
    "#     try:\n",
    "#         roster_url_final = [x for x in roster_urls if x.startswith('https://public.era.nih.gov/pubroster/preRosIndex')]\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "#     rosters = str(roster_url_final)\n",
    "#     rosters = rosters.split(\"'\")\n",
    "#     rosters = [x for x in rosters if x.startswith('https://public.era.nih.gov/pubroster/preRosIndex')]\n",
    "\n",
    "#     for roster in rosters:\n",
    "            \n",
    "#         driver.get(roster)\n",
    "\n",
    "#         html = driver.page_source\n",
    "#         soup = BeautifulSoup(html)\n",
    "\n",
    "#         title_info = []\n",
    "#         for center_tag in soup.find_all('center'):\n",
    "#             snippet = center_tag.text\n",
    "#             snippet = snippet.split('\\n')\n",
    "#             snippet = [i.strip() for i in snippet if i]\n",
    "#             title_info = title_info + snippet\n",
    "            \n",
    "#         blocks = soup.find_all('p')\n",
    "#         try:\n",
    "#             del blocks[0] # deletes block that doesn't contain member info\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "#         for block in blocks:\n",
    "#             block = str(block).replace('\\n',\"\")\n",
    "#             block = block.replace('\\t',\"\")\n",
    "#             block = block.replace('</p>',\"\")\n",
    "#             block = block.replace('</font>',\"\")\n",
    "#             block = block.replace('\\xa0',\" \")\n",
    "#             block = block.replace(\"    \", \"\")\n",
    "#             block = block.replace('<p><font color=\"Navy\" size=\"2\"><font color=\"Navy\" size=\"2\">',\"\")\n",
    "#             block = block.split('<br/>')\n",
    "#             member_list.append(title_info[0:3] + block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# June 2024:\n",
    "# Extracts names of reviewers of standing study sections from rosters on CSR's website \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "source_url = 'https://public.csr.nih.gov/StudySections/StandingStudySections'\n",
    "response = requests.get(source_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "SS_urls = ['https://public.csr.nih.gov' + x.get('href') for x in table.find_all('a') if '/StudySections' in x.get('href')]\n",
    "\n",
    "member_list = []\n",
    "\n",
    "# scrape urls corresponding to each study section's meetings:\n",
    "for SS_url in SS_urls:\n",
    "    response = requests.get(SS_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    try:\n",
    "        roster_urls = [x.get('href') for x in soup.find_all('a') if '/pubroster/preRosIndex' in x.get('href')]\n",
    "    except: # some sections do not have rosters published (new and haven't met yet, etc)\n",
    "        continue\n",
    "\n",
    "    # scrape info from each meeting's published roster:\n",
    "    for roster_url in roster_urls:\n",
    "        response = requests.get(roster_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # print(soup)\n",
    "\n",
    "        # Extract meeting information\n",
    "        SS = soup.find('h2').get_text()\n",
    "        SS = SS.replace('\\n','')\n",
    "        roster_header = soup.find('center')\n",
    "        abbrv_and_date = [x.get_text(separator=\"\\n\").strip() for x in roster_header.find_all('b')]\n",
    "        abbrv = abbrv_and_date[0] # 0 is SS abreviation, 1 is start date (and maybe end date, text meeting roster)\n",
    "        startdate = abbrv_and_date[1].replace('-', '').split('\\n')[0].strip()\n",
    "\n",
    "        # Extract roster information\n",
    "        chunks = soup.find_all('p')\n",
    "        for chunk in chunks[1:-2]:\n",
    "            chunk = chunk.get_text(separator=\"---\").replace('\\t', '').replace('\\n', ' ').replace('*', '')\n",
    "            chunk = [x.strip() for x in chunk.split('---') if x]\n",
    "            chunk_zip = chunk[-1].split(' ')[-1]\n",
    "\n",
    "            entry = [SS] + [abbrv] + [startdate] + [chunk_zip] + chunk\n",
    "            member_list.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rosters for later re-processing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os.path\n",
    "\n",
    "source_url = 'https://public.csr.nih.gov/StudySections/StandingStudySections'\n",
    "response = requests.get(source_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find('table')\n",
    "SS_urls = ['https://public.csr.nih.gov' + x.get('href') for x in table.find_all('a') if '/StudySections' in x.get('href')]\n",
    "\n",
    "roster_path = 'C:\\\\Users\\\\kjj326\\OneDrive - The University of Texas at Austin\\\\study_section_members\\\\rosters'\n",
    "\n",
    "# scrape urls corresponding to each study section's meetings:\n",
    "for SS_url in SS_urls:\n",
    "    response = requests.get(SS_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    try:\n",
    "        roster_urls = [x.get('href') for x in soup.find_all('a') if '/pubroster/preRosIndex' in x.get('href')]\n",
    "    except: # some sections do not have rosters published (new and haven't met yet, etc)\n",
    "        continue\n",
    "\n",
    "    for roster_url in roster_urls:\n",
    "        response = requests.get(roster_url)\n",
    "        filename = os.path.join(roster_path, roster_url[-24:] + \".txt\",)\n",
    "        with open(filename, \"w\") as file:\n",
    "            file.write(response.text)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export member list to CSV\n",
    "import pandas as pd # pandas is overkill but I want to learn it dang it\n",
    "from datetime import datetime\n",
    "\n",
    "my_df = pd.DataFrame(member_list)\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "my_df.to_csv(today+'_study section members.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate all member list data:\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# need to revise to handle n files\n",
    "files = ['20220408_study section members.csv', '20220920_study section members.csv', '20230621_study section members.csv', '20240621_study section members.csv']\n",
    "\n",
    "df = pd.read_csv(files[0])\n",
    "for file in files[1:]:\n",
    "    df = pd.concat([df, pd.read_csv(file)])\n",
    "\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "df.to_csv(today+'_all reviewers.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up names in member list data (remove degrees)\n",
    "import pandas as pd\n",
    "\n",
    "file = '20240624_study section members.csv'\n",
    "\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# strategy: delete everything after the 2nd comma (1st comma used in lastname, firstname)\n",
    "def find_nth(haystack: str, needle: str, n: int) -> int:\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "data[\"Name\"] = [name[:find_nth(name, ',', 2)] for name in data[\"Name\"]]\n",
    "\n",
    "data.to_csv(file[:-4]+'_cleaned.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter reviewer list by UT affiliation, using list from NIH of active UT accounts (Kristen pulled from Commons)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Import with all names uppercase for easiest matching\n",
    "UT_PIs = pd.DataFrame(pd.read_csv('20240624_NIH Commons UT Austin PI list ACTIVE.csv')[\"Name\"].str.upper()) # just grab Name column\n",
    "all_reviewers = pd.read_csv('20240621_study section members_cleaned.csv') # all columns (meeting info)\n",
    "\n",
    "# Output: indices of UT-affiliated names within the all_reviewers doc, so that I can keep info about the meeting they were part of\n",
    "# for each UT PI, find them in the reviewers list and return the indices. Add them to a running list of indices to pull out of the reviewers list\n",
    "keys = list(UT_PIs.columns.values)\n",
    "i1 = all_reviewers.set_index(keys).index\n",
    "i2 = UT_PIs.set_index(keys).index\n",
    "UT_reviewers = all_reviewers[i1.isin(i2)]\n",
    "\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "UT_reviewers.to_csv(today+'_UT NIH reviewers.csv', index=False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter reviewer list by UT affiliation, using list from NIH of active UT accounts (Kristen pulled from Commons)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "zip_list = [str(item) for item in list(range(78701, 78770))] + ['787121140', '787121229', '787120292']\n",
    "Austin_Zipcodes = pd.DataFrame(zip_list, columns=['Zipcode'])\n",
    "all_reviewers = pd.read_csv('20240624_study section members_cleaned.csv') # all columns (meeting info)\n",
    "\n",
    "# Output: indices of UT-affiliated names within the all_reviewers doc, so that I can keep info about the meeting they were part of\n",
    "# for each UT PI, find them in the reviewers list and return the indices. Add them to a running list of indices to pull out of the reviewers list\n",
    "keys = list(Austin_Zipcodes.columns.values)\n",
    "i1 = all_reviewers.set_index(keys).index\n",
    "i2 = Austin_Zipcodes.set_index(keys).index\n",
    "Austin_reviewers = all_reviewers[i1.isin(i2)]\n",
    "\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "Austin_reviewers.to_csv(today+'_Austin NIH reviewers.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "475190006f6fae67546bee186177145b8248021cba5e24e24bb39c6a86ad8326"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
